{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c8c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# package for fine tune\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from datasets import Dataset, load_metric, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score,mean_squared_error,r2_score,accuracy_score,balanced_accuracy_score,roc_curve,auc,f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, log_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdae4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# package for performance specifically\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "sns.set(rc={'figure.figsize':(4,4)},style='ticks',font=\"Arial\", font_scale=1)\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b96a5",
   "metadata": {},
   "source": [
    "# 1 Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e418dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Datasets\n",
    "dfFLIP = pd.read_csv(\"C:/Users/replace_with_your_data.csv\",header=0)\n",
    "df2017 = dfFLIP.loc[dfFLIP[\"FLIP_year\"]==2017].reset_index(drop=True)\n",
    "df2013 = dfFLIP.loc[dfFLIP[\"FLIP_year\"]==2013].reset_index(drop=True) # more years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da6644",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data2017 = df2017\n",
    "data2017 = data2017.loc[data2017['Ingredients'].notna(),]\n",
    "data2017 = data2017.loc[data2017['NOVA'].notna(),]\n",
    "data2017 = data2017.loc[data2017['NOVA']!=\"not_avaliable\",] \n",
    "data2017['NOVA'] = pd.to_numeric(data2017['NOVA'],errors='coerce')\n",
    "display(data2017.shape)\n",
    "\n",
    "df2017b = data2017[['Ingredients','NOVA']] # \"ID\"\n",
    "df2017b.columns = ['text','label']\n",
    "df2017b['label'] = df2017b['label'] - 1 # Make sure that labels start at 0\n",
    "df2017b['label'] = df2017b['label'].astype(int) # And labels are integers\n",
    "display(df2017b.head(5))\n",
    "print(df2017b['label'].unique())\n",
    "#train, test = train_test_split(df2017b, test_size=0.3, random_state=3456)\n",
    "train, test = train_test_split(df2017b, test_size=0.2, random_state=3456)\n",
    "train, valid = train_test_split(train, test_size=0.125, random_state=3456) # 0.125 x 0.8 = 0.1\n",
    "print('train',train.shape), print(train.shape)\n",
    "print('valid',valid.shape), print(valid.shape)\n",
    "print('test',test.shape), print(test.shape)\n",
    "\n",
    "display(train.shape, test.shape)\n",
    "\n",
    "train_dataset = Dataset.from_dict(train)\n",
    "test_dataset = Dataset.from_dict(test)\n",
    "my_dataset_dict = DatasetDict({\"train\":train_dataset,\"test\":test_dataset})\n",
    "my_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56a8775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d20c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics2(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    bacc = balanced_accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'balanced_accuracy_score': bacc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba622b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Train the model\n",
    "# Tokenize_final \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
    "tokenized_datasets1 = my_dataset_dict.map(tokenize_function, batched=True)\n",
    "# create a smaller subset of the full dataset to fine-tune on to reduce the time it takes:\n",
    "small_train_dataset1 = tokenized_datasets1[\"train\"]#.shuffle(seed=1234).select(range(1000))\n",
    "small_eval_dataset1 = tokenized_datasets1[\"test\"]#.shuffle(seed=1234).select(range(1000))\n",
    "model1 = AutoModelForSequenceClassification.from_pretrained('sentence-transformers/multi-qa-MiniLM-L6-cos-v1', num_labels=4)\n",
    "# Training hyperparameters\n",
    "training_args1 = TrainingArguments(disable_tqdm=False,output_dir=\"C:/Users/your_location\", evaluation_strategy=\"epoch\", num_train_epochs=10)\n",
    "# Trainer\n",
    "trainer1 = Trainer(\n",
    "    model=model1, # model\n",
    "    args=training_args1,\n",
    "    train_dataset=small_train_dataset1, # dataset train\n",
    "    eval_dataset=small_eval_dataset1, # dataset evalualtion\n",
    "    compute_metrics=compute_metrics2)\n",
    "trainer1.train()\n",
    "# save and import the model\n",
    "model1.save_pretrained(\"C:/Users/location_of_your_saved_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3457a20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use the saved model  (everytime when re-open)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
    "tokenized_datasets1 = my_dataset_dict.map(tokenize_function, batched=True)\n",
    "# create a smaller subset of the full dataset to fine-tune on to reduce the time it takes:\n",
    "small_train_dataset1 = tokenized_datasets1[\"train\"]#.shuffle(seed=1234).select(range(1000))\n",
    "small_eval_dataset1 = tokenized_datasets1[\"test\"]#.shuffle(seed=1234).select(range(1000))\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained('C:/Users/location_of_your_saved_model/', num_labels=4) ### change\n",
    "# Training hyperparameters\n",
    "training_args1 = TrainingArguments(disable_tqdm=False,output_dir=\"C:/Users/your_location\", evaluation_strategy=\"epoch\",num_train_epochs=10)\n",
    "# Trainer\n",
    "trainer1 = Trainer(\n",
    "    model=model2, # model\n",
    "    args=training_args1,\n",
    "    train_dataset=small_train_dataset1, # dataset train\n",
    "    eval_dataset=small_eval_dataset1, # dataset evalualtion\n",
    "    compute_metrics=compute_metrics2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea6205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### classification_report\n",
    "y_pred = trainer1.predict(tokenized_datasets1[\"test\"])\n",
    "y_true = tokenized_datasets1[\"test\"]['label']\n",
    "predictions = tf.nn.softmax(y_pred.predictions) #, labels=clf.classes_\n",
    "pred = np.argmax(predictions, 1)\n",
    "cm=confusion_matrix(y_true, pred)\n",
    "print(cm)\n",
    "\n",
    "classification_report(y_true, pred)\n",
    "print(classification_report(y_true, pred))\n",
    "\n",
    "### normalized Confusion Matrix\n",
    "c = disp.confusion_matrix\n",
    "normed_c = (c.T / c.astype(np.float).sum(axis=1)).T\n",
    "normed_c\n",
    "normed_c=np.round(normed_c,2) # change digits to 2\n",
    "\n",
    "title  = \"Normalized Confusion Matrix \\n \"\n",
    "disp2 = ConfusionMatrixDisplay(normed_c,\n",
    "                              display_labels=['1','2','3','4'])\n",
    "\n",
    "disp2.plot(cmap=plt.cm.Blues)\n",
    "disp2.ax_.set_title(title, fontsize=13,fontweight=\"bold\") # ,\n",
    "disp2.ax_.set_xlabel('Predicted NOVA Group', fontsize=13)\n",
    "disp2.ax_.set_ylabel('True NOVA Group', fontsize=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86978dc8",
   "metadata": {},
   "source": [
    "# 2 Predict new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c316cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import new data\n",
    "test_xFLIP2013 = df2013\n",
    "test_xFLIP2013 = test_xFLIP2013.loc[:,[\"ID\",\"Ingredients\"]]\n",
    "test_xFLIP2013 =test_xFLIP2013.dropna(axis=0, subset=[\"Ingredients\"])\n",
    "\n",
    "test = test_xFLIP2013\n",
    "print(test.shape)\n",
    "test = test[['Ingredients']]\n",
    "test.columns=['text']\n",
    "\n",
    "FLIP2013ft=test\n",
    "test_df = Dataset.from_dict(FLIP2013ft).map(tokenize_function, batched=True)\n",
    "y_pred = trainer1.predict(test_df)\n",
    "predictions = tf.nn.softmax(y_pred.predictions) #, labels=clf.classes_\n",
    "pred = np.argmax(predictions, 1)\n",
    "\n",
    "# Save predicted data\n",
    "test_xFLIP2013[\"Pred_finetune\"]=pred+1\n",
    "test_xFLIP2013.to_csv('C:/Users/your_location.csv',index=True)\n",
    "print(test_xFLIP2013.Pred_finetune.value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parser-env",
   "language": "python",
   "name": "parser-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
